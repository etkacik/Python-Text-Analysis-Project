{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#BEST 10: read file, find top #10 words for each corporate account\n",
    "\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "import pathlib\n",
    "import string\n",
    "\n",
    "path = pathlib.Path (\"data/top-10\")\n",
    "files = path.glob(\"*.txt\")\n",
    "\n",
    "def get_function_words():\n",
    "    fhand = open(\"data/function_words.txt\")\n",
    "    result = list()\n",
    "    for line in fhand:\n",
    "        result.append(line.strip())\n",
    "    return result\n",
    "\n",
    "fw = get_function_words()\n",
    "\n",
    "\n",
    "def count_type(filename):\n",
    "    fin = open (filename, errors=\"ignore\")\n",
    "\n",
    "    d = {}\n",
    "\n",
    "\n",
    "    text = fin.read()\n",
    "    words = text.split()\n",
    "    for w in words: \n",
    "        w = w.lower()\n",
    "        \n",
    "        if w in d.keys():\n",
    "            d[w] = d.get(w,0) + 1\n",
    "        else: \n",
    "            d[w] = 1\n",
    "    \n",
    "    return d\n",
    "\n",
    "\n",
    "def top_10_words(filename):\n",
    "    d = count_type(filename)\n",
    "    lst = list(d.items())\n",
    "\n",
    "    new_lst = []\n",
    "    for key,val in lst:\n",
    "        \n",
    "        if key in fw: \n",
    "            continue\n",
    "        \n",
    "        else: \n",
    "            t = (val, key)\n",
    "            new_lst.append (t)\n",
    "\n",
    "    new_lst.sort(reverse = True)\n",
    "\n",
    "    result = []\n",
    "    for t in new_lst[:10]:\n",
    "        result.append(t[1])\n",
    "    return result\n",
    "\n",
    "\n",
    "# ---print top 10 words for each file\n",
    "for filename in files:\n",
    "    print (filename)\n",
    "    pp.pprint (top_10_words(filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/bottom-10/sprouts.txt\n",
      "[   'hi',\n",
      "    'thanks',\n",
      "    'hope',\n",
      "    \"we're\",\n",
      "    'reaching',\n",
      "    'out.',\n",
      "    \"we'll\",\n",
      "    'there,',\n",
      "    'thank',\n",
      "    'sorry']\n",
      "data/bottom-10/cvs.txt\n",
      "[   'hi',\n",
      "    'thank',\n",
      "    'team',\n",
      "    'please',\n",
      "    'cvs',\n",
      "    'there,',\n",
      "    'dm',\n",
      "    'hear',\n",
      "    'you.',\n",
      "    'phone']\n",
      "data/bottom-10/frontier.txt\n",
      "[   'direct',\n",
      "    'send',\n",
      "    'please',\n",
      "    \"we're\",\n",
      "    'sorry',\n",
      "    'message',\n",
      "    'message,',\n",
      "    'get',\n",
      "    'hi,',\n",
      "    'apologize']\n",
      "data/bottom-10/express-scripts.txt\n",
      "[   'number,',\n",
      "    'member',\n",
      "    'expressrxhelp@express-scripts.com',\n",
      "    'email',\n",
      "    'please',\n",
      "    'phone',\n",
      "    'name,',\n",
      "    'date',\n",
      "    'birth',\n",
      "    'handle']\n",
      "data/bottom-10/xerox.txt\n",
      "[   'xerox',\n",
      "    'print',\n",
      "    'services',\n",
      "    'metallic',\n",
      "    'holiday',\n",
      "    'business',\n",
      "    '#beyondcmyk',\n",
      "    \"xerox's\",\n",
      "    \"we're\",\n",
      "    'using']\n",
      "data/bottom-10/dish.txt\n",
      "[   'holiday',\n",
      "    'demand.',\n",
      "    'movies',\n",
      "    'dish',\n",
      "    'available',\n",
      "    \"we've\",\n",
      "    'watch',\n",
      "    'vince',\n",
      "    'vaughn',\n",
      "    'tonight']\n",
      "data/bottom-10/dxc-tech.txt\n",
      "[   'read',\n",
      "    '#dxcpartners',\n",
      "    'security',\n",
      "    '#reinvent',\n",
      "    's',\n",
      "    'new',\n",
      "    'learn',\n",
      "    'dxc',\n",
      "    'customers',\n",
      "    'threat']\n",
      "data/bottom-10/family-dollar.txt\n",
      "[   'thank',\n",
      "    'you.',\n",
      "    'family',\n",
      "    'dm',\n",
      "    'dollar',\n",
      "    'bringing',\n",
      "    'attention.',\n",
      "    'appropriate',\n",
      "    'team',\n",
      "    'please']\n",
      "data/bottom-10/aecom.txt\n",
      "[   'ceo',\n",
      "    'professionals',\n",
      "    'bridge',\n",
      "    '#thefutureisnow',\n",
      "    '#socialimpact',\n",
      "    'young',\n",
      "    'wood,',\n",
      "    'wong,',\n",
      "    'washington',\n",
      "    'vital']\n",
      "data/bottom-10/dillards.txt\n",
      "[   'shop',\n",
      "    'here:',\n",
      "    \"women's\",\n",
      "    'sweaters',\n",
      "    'ugg',\n",
      "    'slippers',\n",
      "    'year!',\n",
      "    'worlds',\n",
      "    'words:',\n",
      "    'wishlist!']\n"
     ]
    }
   ],
   "source": [
    "#WORST 10: read file, find top #10 words for each corporate account\n",
    "\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "import pathlib\n",
    "import string\n",
    "\n",
    "path = pathlib.Path (\"data/bottom-10\")\n",
    "files = path.glob(\"*.txt\")\n",
    "\n",
    "def get_function_words():\n",
    "    fhand = open(\"data/function_words.txt\")\n",
    "    result = list()\n",
    "    for line in fhand:\n",
    "        result.append(line.strip())\n",
    "    return result\n",
    "\n",
    "fw = get_function_words()\n",
    "\n",
    "\n",
    "def count_type(filename):\n",
    "    fin = open (filename, errors=\"ignore\")\n",
    "\n",
    "    d = {}\n",
    "\n",
    "    text = fin.read()\n",
    "    words = text.split()\n",
    "    for w in words: \n",
    "        w = w.lower()\n",
    "        \n",
    "        if w in d.keys():\n",
    "            d[w] = d.get(w,0) + 1\n",
    "        else: \n",
    "            d[w] = 1\n",
    "    \n",
    "    return d\n",
    "\n",
    "\n",
    "def top_10_words(filename):\n",
    "    d = count_type(filename)\n",
    "    lst = list(d.items())\n",
    "\n",
    "    new_lst = []\n",
    "    for key,val in lst:\n",
    "        \n",
    "        if key in fw: \n",
    "            continue\n",
    "        \n",
    "        else: \n",
    "            t = (val, key)\n",
    "            new_lst.append (t)\n",
    "\n",
    "    new_lst.sort(reverse = True)\n",
    "\n",
    "    result = []\n",
    "    for t in new_lst[:10]:\n",
    "        result.append(t[1])\n",
    "    return result\n",
    "\n",
    "\n",
    "# ---print top 10 words for each file\n",
    "for filename in files:\n",
    "    print (filename)\n",
    "    pp.pprint (top_10_words(filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words among entire corpus in the TOP-10 folder.\n",
      "['see', 'email', \"we'll\", 'send', 'happy', \"we're\", 'twitter@kimptonhotels.com', \"that's\", 'right', 'miss']\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "import pathlib\n",
    "import string\n",
    "\n",
    "path = pathlib.Path (\"data/top-10\")\n",
    "files = path.glob(\"*.txt\")\n",
    "\n",
    "def get_function_words():\n",
    "    fhand = open(\"data/function_words.txt\")\n",
    "    result = list()\n",
    "    for line in fhand:\n",
    "        result.append(line.strip())\n",
    "    return result\n",
    "\n",
    "fw = get_function_words()\n",
    "\n",
    "\n",
    "def analyze_a_corpus_sample(folder):\n",
    "    list_of_top_words = list()\n",
    "\n",
    "    folder_path = pathlib.Path(folder)\n",
    "    files = folder_path.glob('*txt')\n",
    "\n",
    "    all_words = []\n",
    "    for file_path in files:\n",
    "        fhandle = open(file_path)\n",
    "        text = fhandle.read()\n",
    "        words= text.split()\n",
    "        all_words.extend(words)\n",
    "    \n",
    "    d = dict()\n",
    "    for w in words: \n",
    "        w = w.lower()\n",
    "        \n",
    "        if w in d.keys():\n",
    "            d[w] = d.get(w,0) + 1\n",
    "        else: \n",
    "            d[w] = 1\n",
    "\n",
    "    lst = list(d.items()) \n",
    "    new_lst = []\n",
    "    for key,val in lst:\n",
    "        if key in fw: \n",
    "            continue\n",
    "        else: \n",
    "            t = (val, key)\n",
    "            new_lst.append (t)\n",
    "\n",
    "    new_lst.sort(reverse = True)\n",
    "\n",
    "    # collect the top 10 words\n",
    "    result = []\n",
    "    for t in new_lst[:10]:\n",
    "        result.append(t[1])    \n",
    "        \n",
    "    return result\n",
    "\n",
    "x = analyze_a_corpus_sample(\"data/top-10\")\n",
    "print(\"Top 10 words among entire corpus in the TOP-10 folder.\")\n",
    "print (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words among entire corpus in the BOTTOM-10 folder.\n",
      "['shop', 'here:', \"women's\", 'sweaters', 'ugg', 'slippers', 'year!', 'worlds', 'words:', 'wishlist!']\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "import pathlib\n",
    "import string\n",
    "\n",
    "path = pathlib.Path (\"data/bottom-10\")\n",
    "files = path.glob(\"*.txt\")\n",
    "\n",
    "def get_function_words():\n",
    "    fhand = open(\"data/function_words.txt\")\n",
    "    result = list()\n",
    "    for line in fhand:\n",
    "        result.append(line.strip())\n",
    "    return result\n",
    "\n",
    "fw = get_function_words()\n",
    "\n",
    "\n",
    "def analyze_a_corpus_sample(folder):\n",
    "    list_of_top_words = list()\n",
    "\n",
    "    folder_path = pathlib.Path(folder)\n",
    "    files = folder_path.glob('*txt')\n",
    "\n",
    "    all_words = []\n",
    "    for file_path in files:\n",
    "        fhandle = open(file_path)\n",
    "        text = fhandle.read()\n",
    "        words= text.split()\n",
    "        all_words.extend(words)\n",
    "\n",
    "\n",
    "    d = dict()\n",
    "    for w in words: \n",
    "        w = w.lower()\n",
    "        \n",
    "        if w in d.keys():\n",
    "            d[w] = d.get(w,0) + 1\n",
    "        else: \n",
    "            d[w] = 1\n",
    "\n",
    "    lst = list(d.items()) \n",
    "    new_lst = []\n",
    "    for key,val in lst:\n",
    "        if key in fw: \n",
    "            continue\n",
    "        else: \n",
    "            t = (val, key)\n",
    "            new_lst.append (t)\n",
    "\n",
    "    new_lst.sort(reverse = True)\n",
    "\n",
    "    # collect the top 10 words\n",
    "    result = []\n",
    "    for t in new_lst[:10]:\n",
    "        result.append(t[1])    \n",
    "        \n",
    "    return result\n",
    "\n",
    "x = analyze_a_corpus_sample(\"data/bottom-10\")\n",
    "print(\"Top 10 words among entire corpus in the BOTTOM-10 folder.\")\n",
    "print (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST COMPANIES\n",
      "PRO WORDS\n",
      "########\n",
      "[   ('life', 'balance'),\n",
      "    ('work', 'life'),\n",
      "    ('great', 'benefits'),\n",
      "    ('red', 'hat'),\n",
      "    ('great', 'people'),\n",
      "    ('edward', 'jones'),\n",
      "    ('take', 'care'),\n",
      "    ('long', 'term'),\n",
      "    ('tuition', 'reimbursement'),\n",
      "    ('forward', 'thinking')]\n",
      "########\n",
      "########\n",
      "[   ('work', 'life', 'balance'),\n",
      "    ('life', 'balance', 'great'),\n",
      "    ('life', 'balance', 'empathetic'),\n",
      "    ('life', 'balance', 'hey'),\n",
      "    ('life', 'balance', 'low'),\n",
      "    ('life', 'balance', 'stable'),\n",
      "    ('life', 'balance', 'pay'),\n",
      "    ('life', 'balance', 'american'),\n",
      "    ('life', 'balance', 'compassionate'),\n",
      "    ('life', 'balance', 'respectful')]\n",
      "########\n",
      "CON WORDS\n",
      "########\n",
      "[   ('life', 'balance'),\n",
      "    ('red', 'hat'),\n",
      "    ('work', 'life'),\n",
      "    ('long', 'hours'),\n",
      "    ('home', 'office'),\n",
      "    ('red', 'tape'),\n",
      "    ('large', 'corporation'),\n",
      "    ('decision', 'making'),\n",
      "    ('fast', 'paced'),\n",
      "    ('growth', 'opportunities')]\n",
      "########\n",
      "########\n",
      "[   ('work', 'life', 'balance'),\n",
      "    ('life', 'balance', 'mediocre'),\n",
      "    ('life', 'balance', 'stressful'),\n",
      "    ('training', 'life', 'balance'),\n",
      "    ('life', 'balance', 'take'),\n",
      "    ('life', 'balance', 'could'),\n",
      "    ('life', 'balance', 'time'),\n",
      "    ('life', 'balance', 'work'),\n",
      "    ('leaving', 'red', 'hat'),\n",
      "    ('red', 'hat', 'strives')]\n",
      "########\n"
     ]
    }
   ],
   "source": [
    "#------GLASSDOOR --------\n",
    "# best companies : top 10 bigrams/TRIGRAMS\n",
    "\n",
    "import pathlib\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "import string\n",
    "\n",
    "puncts = string.punctuation\n",
    "# print (puncts)\n",
    "\n",
    "def strip_punct(words):\n",
    "    res = \"\"\n",
    "    for word in words:\n",
    "        if word in puncts:  \n",
    "            continue\n",
    "        else:\n",
    "            res = res + word\n",
    "    return res\n",
    "\n",
    "def html_to_text():\n",
    "    html_path = pathlib.Path(\"html_files/top-10\") #pathlib.Path(\"html_files\")\n",
    "    html_files = html_path.glob(\"*.htm\")\n",
    "\n",
    "    list_of_reviews = list()\n",
    "\n",
    "    for fpath in html_files: \n",
    "        \n",
    "        with open(fpath, errors='ignore') as fin:\n",
    "            html_str = fin.read()\n",
    "            soup = bs(html_str, \"html.parser\")           \n",
    "           \n",
    "            review_feed = soup.find(\"div\", { \"id\" : \"ReviewsFeed\" })\n",
    "            if review_feed != None:\n",
    "                reviews = review_feed.find_all(\"div\", {\"class\": \"gdReview\"})\n",
    "            else:\n",
    "                reviews = None\n",
    "\n",
    "            if reviews != None:\n",
    "                for review in reviews:\n",
    "                    pros_or_cons = 'pros'\n",
    "                    header = review.find(\"h2\")\n",
    "                    main = review.find(\"p\", {\"class\": \"mainText\"})\n",
    "                    paragraphs = review(\"p\")\n",
    "                    pros_text = \"\"\n",
    "                    cons_text = \"\"\n",
    "                    text = \"\"\n",
    "                    for p in paragraphs:\n",
    "                        if p.attrs is not None and 'mainText' in p.attrs.get('class', []):\n",
    "                            continue\n",
    "\n",
    "                        parent = p.find_parent('div', {\"class\", \"comment\"})\n",
    "                        if parent is not None:\n",
    "                            break\n",
    "                           \n",
    "                            \n",
    "                        text = p.text                        \n",
    "                        if text == 'Pros':\n",
    "                            pros_or_cons = 'pros'\n",
    "                        elif text == 'Cons':\n",
    "                            pros_or_cons = 'cons'\n",
    "                        else:\n",
    "                            if pros_or_cons == 'pros':\n",
    "                                pros_text += text\n",
    "                            elif pros_or_cons == 'cons':\n",
    "                                cons_text += text\n",
    "                    d = dict()\n",
    "                    d['header'] = header.text\n",
    "                    d['main']    = main.text\n",
    "                    d['pros']    = pros_text\n",
    "                    d['cons']    = cons_text\n",
    "                    list_of_reviews.append(d)\n",
    "\n",
    "    return list_of_reviews\n",
    "                        \n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(\"['\\w'\\u2019]+\") \n",
    "from nltk.tokenize import RegexpTokenizer      \n",
    "from nltk.corpus import webtext\n",
    "from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures, TrigramAssocMeasures\n",
    "from nltk.corpus import stopwords\n",
    "english_stops = stopwords.words('english')     \n",
    "\n",
    "import pprint                                 \n",
    "pp = pprint.PrettyPrinter(indent=4)            \n",
    "\n",
    "reviews = html_to_text()\n",
    "\n",
    "pros_words = []\n",
    "cons_words = []\n",
    "\n",
    "for r in reviews:\n",
    "    pros = r['pros']\n",
    "    words = tokenizer.tokenize(pros)\n",
    "    pros_words.extend(words)\n",
    "    pros_words = list(map(lambda s: s.lower(), pros_words))\n",
    "\n",
    "    \n",
    "    cons = r['cons']\n",
    "    words = tokenizer.tokenize(cons)\n",
    "    cons_words.extend(words)\n",
    "    cons_words = list(map(lambda s: s.lower(), cons_words))\n",
    "\n",
    "\n",
    "\n",
    "min_length = 3\n",
    "def filter_stops(w):\n",
    "    if len(w) < min_length or w in english_stops:\n",
    "        return w\n",
    "    return None\n",
    "\n",
    "print (\"BEST COMPANIES\")\n",
    "print (\"PRO WORDS\")\n",
    "\n",
    "bcf = BigramCollocationFinder.from_words(pros_words)\n",
    "bcf.apply_word_filter(filter_stops)  \n",
    "res = bcf.nbest(BigramAssocMeasures.likelihood_ratio, 10)\n",
    "\n",
    "print(\"########\")\n",
    "pp.pprint(res)\n",
    "print(\"########\")\n",
    "\n",
    "\n",
    "bcf = TrigramCollocationFinder.from_words(pros_words)\n",
    "bcf.apply_word_filter(filter_stops)  \n",
    "res = bcf.nbest(TrigramAssocMeasures.likelihood_ratio, 10)\n",
    "\n",
    "print(\"########\")\n",
    "pp.pprint(res)\n",
    "print(\"########\")\n",
    "\n",
    "print (\"CON WORDS\")\n",
    "bcf = BigramCollocationFinder.from_words(cons_words)\n",
    "bcf.apply_word_filter(filter_stops)  \n",
    "res = bcf.nbest(BigramAssocMeasures.likelihood_ratio, 10)\n",
    "\n",
    "print(\"########\")\n",
    "pp.pprint(res)\n",
    "print(\"########\")\n",
    "\n",
    "\n",
    "bcf = TrigramCollocationFinder.from_words(cons_words)\n",
    "bcf.apply_word_filter(filter_stops)  \n",
    "res = bcf.nbest(TrigramAssocMeasures.likelihood_ratio, 10)\n",
    "\n",
    "print(\"########\")\n",
    "pp.pprint(res)\n",
    "print(\"########\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORST COMPANIES\n",
      "pro words\n",
      "########\n",
      "[   ('life', 'balance'),\n",
      "    ('fast', 'paced'),\n",
      "    ('flexible', 'schedule'),\n",
      "    ('work', 'life'),\n",
      "    ('great', 'people'),\n",
      "    ('flexible', 'hours'),\n",
      "    ('good', 'pay'),\n",
      "    ('customer', 'service'),\n",
      "    ('good', 'benefits'),\n",
      "    ('laid', 'back')]\n",
      "########\n",
      "########\n",
      "[   ('work', 'life', 'balance'),\n",
      "    ('life', 'balance', 'decent'),\n",
      "    ('life', 'balance', 'love'),\n",
      "    ('life', 'balance', 'many'),\n",
      "    ('life', 'balance', 'lots'),\n",
      "    ('life', 'balance', 'work'),\n",
      "    ('life', 'balance', 'good'),\n",
      "    ('fast', 'paced', 'environment'),\n",
      "    ('fast', 'paced', 'startup'),\n",
      "    ('ideas', 'fast', 'paced')]\n",
      "########\n",
      "con words\n",
      "########\n",
      "[   ('long', 'hours'),\n",
      "    ('life', 'balance'),\n",
      "    ('upper', 'management'),\n",
      "    ('low', 'pay'),\n",
      "    ('work', 'life'),\n",
      "    ('high', 'turnover'),\n",
      "    ('new', 'hires'),\n",
      "    ('poor', 'management'),\n",
      "    ('store', 'manager'),\n",
      "    ('family', 'dollar')]\n",
      "########\n",
      "########\n",
      "[   ('work', 'life', 'balance'),\n",
      "    ('long', 'hours', 'job'),\n",
      "    ('long', 'hours', 'tedious'),\n",
      "    ('mundane', 'long', 'hours'),\n",
      "    ('places', 'long', 'hours'),\n",
      "    ('work', 'long', 'hours'),\n",
      "    ('addressed', 'long', 'hours'),\n",
      "    ('liked', 'long', 'hours'),\n",
      "    ('completely', 'long', 'hours'),\n",
      "    ('long', 'hours', 'weekends')]\n",
      "########\n"
     ]
    }
   ],
   "source": [
    "#------GLASSDOOR --------\n",
    "# worst companies : top 10 bigrams/TRIGRAMS\n",
    "\n",
    "def html_to_text():\n",
    "    html_path = pathlib.Path(\"html_files/bottom-10\") \n",
    "    html_files = html_path.glob(\"*.htm\")\n",
    "\n",
    "    list_of_reviews = list()\n",
    "\n",
    "    for fpath in html_files: \n",
    "        \n",
    "        with open(fpath, errors='ignore') as fin:\n",
    "            html_str = fin.read()\n",
    "            soup = bs(html_str, \"html.parser\")           \n",
    "           \n",
    "            review_feed = soup.find(\"div\", { \"id\" : \"ReviewsFeed\" }) \n",
    "            if review_feed != None:\n",
    "                reviews = review_feed.find_all(\"div\", {\"class\": \"gdReview\"})\n",
    "            else:\n",
    "                reviews = None\n",
    "\n",
    "            if reviews != None:\n",
    "                for review in reviews:\n",
    "                    pros_or_cons = 'pros'\n",
    "                    header = review.find(\"h2\")\n",
    "                    main = review.find(\"p\", {\"class\": \"mainText\"})\n",
    "                    paragraphs = review(\"p\")\n",
    "                    pros_text = \"\"\n",
    "                    cons_text = \"\"\n",
    "                    text = \"\"\n",
    "                    for p in paragraphs:\n",
    "                        if p.attrs is not None and 'mainText' in p.attrs.get('class', []):\n",
    "                            continue\n",
    "                        \n",
    "                        parent = p.find_parent('div', {\"class\", \"comment\"})\n",
    "                        if parent is not None:\n",
    "                            break\n",
    "                            \n",
    "                            \n",
    "                        text = p.text                        \n",
    "                        if text == 'Pros':\n",
    "                            pros_or_cons = 'pros'\n",
    "                        elif text == 'Cons':\n",
    "                            pros_or_cons = 'cons'\n",
    "                        else:\n",
    "                            if pros_or_cons == 'pros':\n",
    "                                pros_text += text\n",
    "                            elif pros_or_cons == 'cons':\n",
    "                                cons_text += text\n",
    "                    d = dict()\n",
    "                    d['header'] = header.text\n",
    "                    d['main']    = main.text\n",
    "                    d['pros']    = pros_text\n",
    "                    d['cons']    = cons_text\n",
    "                    list_of_reviews.append(d)\n",
    "\n",
    "    return list_of_reviews\n",
    "\n",
    "#------\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(\"['\\w'\\u2019]+\") \n",
    "from nltk.tokenize import RegexpTokenizer      \n",
    "from nltk.corpus import webtext\n",
    "from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures, TrigramAssocMeasures\n",
    "from nltk.corpus import stopwords\n",
    "english_stops = stopwords.words('english')     \n",
    "\n",
    "import pprint                                  \n",
    "pp = pprint.PrettyPrinter(indent=4)            \n",
    "\n",
    "\n",
    "reviews = html_to_text() \n",
    "\n",
    "pros_words = []\n",
    "cons_words = []\n",
    "\n",
    "for r in reviews:\n",
    "    pros = r['pros']\n",
    "    words = tokenizer.tokenize(pros)\n",
    "    pros_words.extend(words)\n",
    "    pros_words = list(map(lambda s: s.lower(), pros_words))\n",
    "\n",
    "    \n",
    "    cons = r['cons']\n",
    "    words = tokenizer.tokenize(cons)\n",
    "    cons_words.extend(words)\n",
    "    cons_words = list(map(lambda s: s.lower(), cons_words))\n",
    "\n",
    "    \n",
    "\n",
    "min_length = 3\n",
    "def filter_stops(w):\n",
    "\n",
    "    if len(w) < min_length or w in english_stops:\n",
    "        return w\n",
    "    return None\n",
    "\n",
    "print (\"WORST COMPANIES\")\n",
    "\n",
    "\n",
    "print (\"pro words\")\n",
    "bcf = BigramCollocationFinder.from_words(pros_words)\n",
    "bcf.apply_word_filter(filter_stops)  \n",
    "res = bcf.nbest(BigramAssocMeasures.likelihood_ratio, 10)\n",
    "\n",
    "print(\"########\")\n",
    "pp.pprint(res)\n",
    "print(\"########\")\n",
    "\n",
    "\n",
    "bcf = TrigramCollocationFinder.from_words(pros_words)\n",
    "bcf.apply_word_filter(filter_stops)  \n",
    "res = bcf.nbest(TrigramAssocMeasures.likelihood_ratio, 10)\n",
    "\n",
    "print(\"########\")\n",
    "pp.pprint(res)\n",
    "print(\"########\")\n",
    "\n",
    "print (\"con words\")\n",
    "bcf = BigramCollocationFinder.from_words(cons_words)\n",
    "bcf.apply_word_filter(filter_stops)  \n",
    "res = bcf.nbest(BigramAssocMeasures.likelihood_ratio, 10)\n",
    "\n",
    "print(\"########\")\n",
    "pp.pprint(res)\n",
    "print(\"########\")\n",
    "\n",
    "\n",
    "bcf = TrigramCollocationFinder.from_words(cons_words)\n",
    "bcf.apply_word_filter(filter_stops)  \n",
    "res = bcf.nbest(TrigramAssocMeasures.likelihood_ratio, 10)\n",
    "\n",
    "print(\"########\")\n",
    "pp.pprint(res)\n",
    "print(\"########\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
